{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_eb48b_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_eb48b_row0_col0\" class=\"data row0 col0\" >session_id</td>\n",
       "      <td id=\"T_eb48b_row0_col1\" class=\"data row0 col1\" >4603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_eb48b_row1_col0\" class=\"data row1 col0\" ># Transactions</td>\n",
       "      <td id=\"T_eb48b_row1_col1\" class=\"data row1 col1\" >19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_eb48b_row2_col0\" class=\"data row2 col0\" ># Items</td>\n",
       "      <td id=\"T_eb48b_row2_col1\" class=\"data row2 col1\" >1893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_eb48b_row3_col0\" class=\"data row3 col0\" >Ignore Items</td>\n",
       "      <td id=\"T_eb48b_row3_col1\" class=\"data row3 col1\" >None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d29d44edf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: This file will mine the association rules form each cluster.\n",
    "Input: a csv file to be inspected\n",
    "Return: none\n",
    "Output:\n",
    "Lessons learned:\n",
    "    1) Don't mix up Anaconda Pycharm and Pycharm, it messes up the libraries\n",
    "    2) The data should be split into separate tables or data frames based on\n",
    "        cluster assignment before processing.  The ARM algorithm does not consider cluster,\n",
    "        so it should be run on each cluster separately.\n",
    "\"\"\"\n",
    "# source: https://www.pycaret.org/tutorials/html/ARUL101.html\n",
    "from pycaret.arules import *\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Purpose: This function sets the working directory and loads the data in a pandas dataframe\n",
    "    :return: a pandas dataframe\n",
    "    \"\"\"\n",
    "    # set working directory and csv file path\n",
    "    path = \"C:\\\\git_local\\\\WGU_Capstone_Python\"\n",
    "    os.chdir(path)\n",
    "    file = 'data\\\\cleaned_data_with_clusters.csv'\n",
    "    # pycaret's get_data results in a an error, pycaret also accepts pandas data frame\n",
    "    # I think in this case we want to include the headers as pycaret setup() uses headers\n",
    "    return pandas.read_csv(file)\n",
    "\n",
    "\n",
    "def setup_model(data):\n",
    "    \"\"\"\n",
    "        The setup() function initializes the environment in pycaret and transforms the transactional dataset into\n",
    "        a shape that is acceptable to the apriori algorithm. It requires two mandatory parameters: transaction_id\n",
    "        which is the name of the column representing the transaction id and will be used to pivot the matrix,\n",
    "        and item_id which is the name of the column used for the creation of rules. Normally, this will be the\n",
    "        variable of interest. You can also pass the optional parameter ignore_items to ignore certain values when\n",
    "        creating rules.\n",
    "\n",
    "        In my case the transaction_id would equal customer_id and the item_id would equal product_id.\n",
    "        customer_id can identify combined purchases and product_id identifies the product.\n",
    "    \"\"\"\n",
    "    setup0 = setup(\n",
    "        data=data,\n",
    "        transaction_id='item_id',\n",
    "        item_id='product_id'\n",
    "    )\n",
    "    # create the model (association rules)\n",
    "    # this step is very problematic, and crashes if dataset is too large\n",
    "    # it also crashes based on input\n",
    "    # shouldn't machine learning libraries handle large data by default?\n",
    "    model = create_model()\n",
    "    print('model shape:')\n",
    "    print(model.shape)\n",
    "    #show rules\n",
    "    print(model.head())\n",
    "\n",
    "\n",
    "\n",
    "data_set = load_data()\n",
    "# split the dataset into subsets based on cluster\n",
    "# cluster0, cluster1, cluster2 = data_set.groupby(data_set['clusters'])\n",
    "cluster0 = data_set[data_set['clusters'] == 0]\n",
    "cluster1 = data_set[data_set['clusters'] == 1]\n",
    "cluster2 = data_set[data_set['clusters'] == 2]\n",
    "setup_model(cluster2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
